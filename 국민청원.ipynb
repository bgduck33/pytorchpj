{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6afc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bdf9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('crawling.csv')\n",
    "#CSV 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac562a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#공백, 특수문자 제거\n",
    "def remove_white_space(text):\n",
    "    text = re.sub(r'[\\t\\r\\n\\f\\v]', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "def remove_special_char(text):\n",
    "    text = re.sub('[^ ㄱ-ㅣ가-힣 0-9]+', ' ', str(text))\n",
    "    return text\n",
    "\n",
    "df.title = df.title.apply(remove_white_space)\n",
    "df.title = df.title.apply(remove_special_char)\n",
    "\n",
    "df.content = df.content.apply(remove_white_space)\n",
    "df.content = df.content.apply(remove_special_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a114223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "#토크나이징, 제목은 형태소, 내용은 명사 \n",
    "okt = Okt()\n",
    "\n",
    "df['title_token'] = df.title.apply(okt.morphs)\n",
    "df['content_token'] = df.content.apply(okt.nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7631ec4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category         object\n",
      "content          object\n",
      "count             int64\n",
      "end              object\n",
      "start            object\n",
      "title            object\n",
      "title_token      object\n",
      "content_token    object\n",
      "token_final      object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['token_final'] = df.title_token + df.content_token\n",
    "\n",
    "df['count'] = df['count'].replace({',' : ''}, regex = True).apply(lambda x : int(x))\n",
    "#정답지 label 을 구성하기 위해 1,786->1786으로 바꾸고 인트형으로 변환\n",
    "print(df.dtypes)\n",
    "\n",
    "df['label'] = df['count'].apply(lambda x: 'Yes' if x>=1000 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae49654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_final</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['서울', '지방', '병무청', '탈의실', '에', '설치', '된', '에'...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['주식시장', '활성화', '및', '소액', '개미', '투자자', '보호', ...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['교정', '기관', '의', '민낮', '일로', '국민', '청원', '신청'...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['미세먼지', '저', '감', '대책', '미세먼지', '심각', '성은', '...</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['악질', '세', '입자', '방지', '를', '위', '한', '세', '입...</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         token_final label\n",
       "0  ['서울', '지방', '병무청', '탈의실', '에', '설치', '된', '에'...    No\n",
       "1  ['주식시장', '활성화', '및', '소액', '개미', '투자자', '보호', ...    No\n",
       "2  ['교정', '기관', '의', '민낮', '일로', '국민', '청원', '신청'...    No\n",
       "3  ['미세먼지', '저', '감', '대책', '미세먼지', '심각', '성은', '...    No\n",
       "4  ['악질', '세', '입자', '방지', '를', '위', '한', '세', '입...   Yes"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_drop = pd.read_csv('df_drop.csv')\n",
    "df_drop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1ee866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop.to_csv(r'C:\\Users\\82108\\딥러닝 프로젝트\\df_drop.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f642e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=42642, vector_size=100, alpha=0.025>\n",
      "[('음주', 0.8594802021980286), ('무면허', 0.8299345970153809), ('뺑소니', 0.8180090188980103), ('살인자', 0.7722511291503906), ('살인죄', 0.7697737812995911), ('형량', 0.7556667923927307), ('전과자', 0.7462077140808105), ('운전자', 0.7455174326896667), ('촉법소년', 0.7346683144569397), ('강력범죄', 0.73124760389328)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "embedding_model = Word2Vec(df_drop['token_final'], \n",
    "                           sg = 1, # skip-gram\n",
    "                           vector_size = 100, \n",
    "                           window = 2, \n",
    "                           min_count = 1, \n",
    "                           workers = 4\n",
    "                           )\n",
    "\n",
    "print(embedding_model)\n",
    "\n",
    "model_result = embedding_model.wv.most_similar(\"음주운전\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94045f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('음주', 0.8594802021980286), ('무면허', 0.8299345970153809), ('뺑소니', 0.8180090188980103), ('살인자', 0.7722511291503906), ('살인죄', 0.7697737812995911), ('형량', 0.7556667923927307), ('전과자', 0.7462077140808105), ('운전자', 0.7455174326896667), ('촉법소년', 0.7346683144569397), ('강력범죄', 0.73124760389328)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embedding_model.wv.save_word2vec_format(r'C:\\Users\\82108\\딥러닝 프로젝트\\petitions_tokens_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(r'C:\\Users\\82108\\딥러닝 프로젝트\\petitions_tokens_w2v') # 모델 로드\n",
    "\n",
    "model_result = loaded_model.most_similar(\"음주운전\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d325d5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다시 킬때는 여기부터\n",
    "from numpy.random import RandomState\n",
    "\n",
    "rng = RandomState()\n",
    "\n",
    "tr = df_drop.sample(frac=0.8, random_state=rng) #랜덤한 인덱스의 데이터 80%를 train dataset에 저장\n",
    "val = df_drop.loc[~df_drop.index.isin(tr.index)] #나머지\n",
    "\n",
    "tr.to_csv(r'C:\\Users\\82108\\딥러닝 프로젝트\\train.csv', index=False, encoding='utf-8-sig')\n",
    "val.to_csv(r'C:\\Users\\82108\\딥러닝 프로젝트\\validation.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57a42429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.data import Field\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('[\\[\\]\\']', '', str(text))\n",
    "    text = text.split(', ') #토근들이 하나의 리스트로 묶여있기 때문에 하나씩 분해\n",
    "    return text\n",
    "\n",
    "TEXT = Field(tokenize=tokenizer)\n",
    "LABEL = Field(sequential = False) #라벨은 순서가 없는 Yes NO 형식이기 때문에 Sequential = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bfb3c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: ['마스크', '를', '1', '재난', '기본소득', '예산', '으로', '국가', '가', '사서', '2', '줄', '서지', '말고', '세대', '별로', '배급', '하면', '국민', '의', '안전', '과', '시간', '을', '보호', '할', '수', '있습니다', '저', '부산', '의사', '요즘', '매일', '어디', '서든', '볼', '수', '풍경', '마스크', '약국', '선', '진료', '소', '줄', '동안', '위험', '비', '효율', '때문', '고안', '드라이브', '스루', '방법', '외국', '약국', '줄', '것', '자신', '보호', '위', '마스크', '사기', '위해', '알', '수', '감염', '위험', '노출', '안', '경제', '활동', '비', '생산', '일', '시간', '모함', '어려움', '가중', '것', '최근', '일부', '지방', '자치', '단체', '전격', '재난', '기본소득', '지급', '결정', '국민', '청원', '재난', '기본소득', '지급', '의견', '동의', '취약', '계층', '마스크', '더', '시간', '마음대로', '낼', '수', '경우', '재난', '기본소득', '누구', '배', '분할', '것', '의논', '합의', '도출', '지급', '꽤', '시간', '소요', '것', '재난', '기본소득', '예산', '당장', '마스크', '국가', '기금', '활용', '그', '나머지', '예산', '경제', '위해', '여러가지', '방법', '강', '구해', '볼', '수', '것', '마스크', '배분', '방법', '최소', '행정', '단위', '별로', '배포', '각', '주민', '그', '쪽', '방문', '거나', '노원구', '공무원', '수고', '주시', '방법', '것', '모든', '사람', '마스크', '구', '수', '안심', '누가', '더', '사재기', '감시', '안해', '시기', '줄', '안', '서도', '자신', '생업', '더욱', '집중', '수', '것', '딩동', '마스크', '노원구', '통장', '집', '배달', '전주시', '지자체', '첫', '재난', '기본소득', '통과', '기사', '보고', '우리', '사회', '이', '위기', '더욱', '더', '수', '것', '생각', '이', '청원'] Yes\n",
      "Validation: ['미세먼지', '저', '감', '대책', '미세먼지', '심각', '성은', '이제', '적극', '대안', '요구', '우리', '일상', '가장', '적극', '수', '것', '모든', '건축물', '외부', '스프링', '쿨러', '설치', '것', '도로', '변', '가로등', '등', '이용', '스프링', '쿨러', '설치', '일정', '농도', '이상', '미세먼지', '발생', '시', '물', '뿌리', '즉시', '미세먼지', '흡착', '수', '옥상', '조경', '건물', '외부', '수직', '정원', '등', '결합', '면', '환경문제', '적극', '해결', '수', '여름', '도', '이상', '온도', '수', '겨울', '미세먼지', '뿐', '도시', '온도', '도', '이상', '수도', '각', '가정', '각', '건물', '수도', '시설', '옥외', '스프링', '쿨러', '설치', '비용', '최소', '만원', '최대로', '예산', '효과', '즉시', '수', '옥상녹화', '수직', '정원', '결합', '금수강산', '대한민국', '다시', '수', '참고', '저', '농업인', '농', '생물학', '생태', '시스템', '공학', '전공', '박사학위', '소유자', '제', '수직', '정원', '인공', '조물', '녹화', '식물', '품종', '개발', '해외', '로열티', '수출', '정원', '관련', '분야', '일도', '미세먼지', '도시', '환경문제', '근본', '대책', '매우', '시간', '노력', '것', '우선', '적극', '적임', '대안', '중', '가장', '실효', '대안', '적극', '검토'] No\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "import re\n",
    "\n",
    "train, validation = TabularDataset.splits(\n",
    "    path = 'data/',\n",
    "    train = 'train.csv',\n",
    "    validation = 'validation.csv',\n",
    "    format = 'csv',\n",
    "    fields = [('text', TEXT), ('label', LABEL)],\n",
    "    skip_header = True #CSV 파일의 첫 행이 칼럼명일 경우 건너뛰기\n",
    ")\n",
    "\n",
    "print(\"Train:\", train[0].text,  train[0].label)\n",
    "print(\"Validation:\", validation[0].text, validation[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86141731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 벡터의 개수와 차원 : torch.Size([38755, 100]) \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "vectors = Vectors(name=\"data/petitions_tokens_w2v\")\n",
    "\n",
    "TEXT.build_vocab(train, vectors = vectors, min_freq = 1, max_size = None) \n",
    "#train 데이터셋의 단어들을 사전 학습된 단어장의 벡터로 초기화\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_iter, validation_iter = BucketIterator.splits(\n",
    "    datasets = (train, validation),\n",
    "    batch_size = 8,\n",
    "    device = device,\n",
    "    sort = False\n",
    ")\n",
    "\n",
    "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29ee0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn   \n",
    "import torch.optim as optim \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class TextCNN(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        \n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)      \n",
    "    \n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        self.relu = nn.ReLU()                \n",
    "        self.dropout = nn.Dropout(0.4)         \n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)     \n",
    "        \n",
    "    def forward(self, x):  \n",
    "      \n",
    "        emb_x = self.embed(x)  #(배치사이즈, 문장 길이, 임베딩 차원)         \n",
    "        emb_x = emb_x.unsqueeze(1)  #(배치사이즈, 1, 문장 길이, 임베딩 차원) \n",
    "        #CNN모델에 적용하기 위해 입력 형태를 3차원 데이터로 바꾸어줌\n",
    "\n",
    "        con_x = [self.relu(conv(emb_x)) for conv in self.convs]       \n",
    "        #self.convs에 있는 module_list를 ebm_x에 각각 적용 후 리스트로 저장 (배치사이즈, 10, 출력길이, 1)짜리 3개\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]    \n",
    "        #풀링레이어 con_x를 1짜리 차원 줄여준 것을 입력으로, 문장길이를 kernel_size로 한다. 출력 - (배치사이즈, 10, 1) 3개\n",
    "        fc_x = torch.cat(pool_x, dim=1) #1차원 방향으로 합치기 (배치사이즈, 30, 1) 1개\n",
    "        fc_x = fc_x.squeeze(-1) #1짜리 마지막 차원 제거해 선형 함수에 넣기 좋은 형태로 만들기\n",
    "        fc_x = self.dropout(fc_x)         \n",
    "\n",
    "        logit = self.fc(fc_x)     \n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ce1056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_itr, optimizer):\n",
    "    \n",
    "    model.train()                               \n",
    "    corrects, train_loss = 0.0,0        \n",
    "    \n",
    "    for batch in train_itr:\n",
    "        \n",
    "        text, target = batch.text, batch.label      \n",
    "        text = torch.transpose(text, 0, 1)          \n",
    "        target.data.sub_(1)                                 \n",
    "        text, target = text.to(device), target.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()                           \n",
    "        logit = model(text)                         \n",
    "    \n",
    "        loss = F.cross_entropy(logit, target)   \n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        \n",
    "        train_loss += loss.item()    \n",
    "        result = torch.max(logit,1)[1] \n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "        \n",
    "    train_loss /= len(train_itr.dataset)\n",
    "    accuracy = 100.0 * corrects / len(train_itr.dataset)\n",
    "\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a6032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, itr):\n",
    "    \n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0, 0\n",
    "\n",
    "    for batch in itr:\n",
    "        \n",
    "        text = batch.text\n",
    "        target = batch.label\n",
    "        text = torch.transpose(text, 0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "\n",
    "    test_loss /= len(itr.dataset) \n",
    "    accuracy = 100.0 * corrects / len(itr.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9835d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN(\n",
      "  (embed): Embedding(38755, 100)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 100), stride=(1, 1))\n",
      "    (1): Conv2d(1, 10, kernel_size=(4, 100), stride=(1, 1))\n",
      "    (2): Conv2d(1, 10, kernel_size=(5, 100), stride=(1, 1))\n",
      "  )\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=30, out_features=2, bias=True)\n",
      ")\n",
      "Train Epoch: 1 \t Loss: 0.08189564037774912 \t Accuracy: 61.80356216430664%\n",
      "Valid Epoch: 1 \t Loss: 0.0773220794439754 \t Accuracy: 66.6819839477539%\n",
      "model saves at 66.6819839477539 accuracy\n",
      "-----------------------------------------------------------------------------\n",
      "Train Epoch: 2 \t Loss: 0.07546763614564705 \t Accuracy: 67.8690414428711%\n",
      "Valid Epoch: 2 \t Loss: 0.0762244074139744 \t Accuracy: 67.00367736816406%\n",
      "model saves at 67.00367736816406 accuracy\n",
      "-----------------------------------------------------------------------------\n",
      "Train Epoch: 3 \t Loss: 0.06685245595123493 \t Accuracy: 73.6358413696289%\n",
      "Valid Epoch: 3 \t Loss: 0.07621472432990284 \t Accuracy: 67.4632339477539%\n",
      "model saves at 67.4632339477539 accuracy\n",
      "-----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN(vocab, 100, 10, [3, 4, 5], 2).to(device)\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_test_acc = -1\n",
    "\n",
    "for epoch in range(1, 3+1):\n",
    " \n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer) \n",
    "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    val_loss, val_acc = evaluate(model, device, validation_iter)\n",
    "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, val_loss, val_acc))\n",
    "        \n",
    "    if val_acc > best_test_acc:\n",
    "        best_test_acc = val_acc\n",
    "        \n",
    "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
    "        torch.save(model.state_dict(), \"TextCNN_Best_Validation\")\n",
    "    \n",
    "    print('-----------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0890e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
